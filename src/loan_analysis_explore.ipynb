{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - Prosper Loan Data Exploration\n",
    "### by Tristen Wallace\n",
    "\n",
    "## Preliminary Wrangling Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import custom_funcs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = '../data/prosperLoanData.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#Data structure\n",
    "print(df.shape)\n",
    "\n",
    "#Preview data\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review variables\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review daterange\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensionality Reduction:** We don't need many of these columns so we will create a subset with only the columns/features that interest us for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varaibles of interest\n",
    "cols = ['LoanStatus', 'Term', 'CreditGrade', 'BorrowerAPR',\n",
    "        'ProsperRating (Alpha)', 'ListingCategory (numeric)', 'BorrowerState', 'Occupation', 'EmploymentStatus',\n",
    "        'IsBorrowerHomeowner', 'GroupKey', 'DebtToIncomeRatio', 'IncomeRange', 'ListingNumber', 'ProsperPrincipalBorrowed',\n",
    "        'ProsperPrincipalOutstanding', 'LoanOriginalAmount', 'LoanOriginationDate', 'MemberKey', 'InvestmentFromFriendsCount', \n",
    "        'InvestmentFromFriendsAmount', 'Investors']\n",
    "\n",
    "df_sub = df[cols]\n",
    "\n",
    "# Descriptive statistics\n",
    "df_sub.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename variables\n",
    "df_sub = df_sub.rename(columns={'ProsperRating (Alpha)':'ProsperRating',\n",
    "                        'ListingCategory (numeric)':'ListingCategory'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df_sub.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.LoanOriginationDate = pd.to_datetime(df.LoanOriginationDate)\n",
    "sns.heatmap(df_sub.set_index('LoanOriginationDate').sort_index().isna().transpose(),\n",
    "            cbar_kws={'label':'Missing Data'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of loans where at least one friend invested\n",
    "df_sub.query('InvestmentFromFriendsCount > 1').describe() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique income ranges\n",
    "df_sub.IncomeRange.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check counts for unique term lengths\n",
    "df_sub.Term.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check counts for unique loan statuses\n",
    "df_sub.LoanStatus.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check counts for unique employment statuses\n",
    "df_sub.EmploymentStatus.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.query('EmploymentStatus == \"Not available\"')['IncomeRange']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique occupation values\n",
    "df_sub.Occupation.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many different occupations?\n",
    "df_sub.Occupation.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare credit grade and Prosper rating\n",
    "print(df_sub.ProsperRating.unique())\n",
    "print(df_sub.CreditGrade.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.ListingCategory.value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Multiple variables contain placeholders for NaN values that avoided detection when searching for missing values. We should replace these values in `ListingCategory`, `IncomeRange`, and `EmploymentStatus` and reassess missing values visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace placeholders with NaN values\n",
    "df_sub.ListingCategory = df_sub.ListingCategory.replace({0:np.nan})\n",
    "df_sub.IncomeRange = df_sub.IncomeRange.replace({'Not displayed':np.nan})\n",
    "df_sub.EmploymentStatus = df_sub.EmploymentStatus.replace({'Not available':np.nan})\n",
    "\n",
    "# Visualize missing values\n",
    "sns.heatmap(df_sub.set_index('LoanOriginationDate').sort_index().isna().transpose(),\n",
    "            cbar_kws={'label':'Missing Data'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After assessing our variables of interest it's clear that we have several cleaning tasks to perform:\n",
    "\n",
    "1. Our analysis doesn't require the granularity present in `LoanStatus`. We can combine the past due category and combine 'FinalPaymentInProgress' with 'Completed'.\n",
    "\n",
    "2. Our analysis doesn't require the granularity present in `Occupation`. We can group occupations into smaller categories using ISCO grouping standards.\n",
    "\n",
    "3. Based on the heatmap and their unique values comparison, we can see the `CreditGrade` and `ProsperRating` store the same variables. The two can be melted into one column using the ranking scheme AA - HR.\n",
    "\n",
    "4. 19.4% of borrowers had exisiting loan balances with Prosper at the time of new loan origination. These pre-existing loan totals are stored in `ProsperPrincipalBorrowed` and `ProsperPrincipalOutstanding`. While in missing value assessment these variables appear to have 80.6% NaN values and qualify for removal, descriptive statistics show that the 19.4% of values have a meaningful value size to warrant keeping them. To deal with the missing values, we can engineer new 'TotalBorrowed' and 'TotalOutstanding' variables by combining the variables above with LoanOriginalAmount. \n",
    "\n",
    "5. Our analysis doesn't require the granularity present in `ListingCategory`. To reduce the categories, we'll keep the top 8 (1,7, 2, 3, 6, 4, 13, 15, 18), and lump the remaining in with 'Other'. \n",
    "\n",
    "6. `ListingCategory` is only missing values from before 08', but we can't drop these rows and it would be misleading to replace the NaN values with a common value. The best option is to leave the missing category as it was.\n",
    "\n",
    "7. `IncomeRange` and `EmploymentStatus` weren't recorded prior to 07'. That date range isn't relavent to our analysis, so we can just drop the missing rows along with the remaining NaN values throughout the dataset\n",
    "\n",
    "6. Remove unnecessary variables:\n",
    "    - Group Key: Contains more than 70% NaN\n",
    "    - IsBorrowerHomeowner, MemberKey: Not needed for our analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df_sub.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove unnecessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.drop(['GroupKey', 'IsBorrowerHomeowner', 'MemberKey'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Group Occupation Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Occupation values to ISCO categories using custom mapp\n",
    "for index, row in clean_df.iterrows():\n",
    "    clean_df.at[index, 'Occupation'] = custom_funcs.map_occupation_to_isco(row['Occupation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isco = ['Professionals', 'Clerical Support', 'Managers', 'Service/Sales',\n",
    "        'Craft Workers', 'Other', 'Technicians', 'Skilled Agr/Forestry',\n",
    "        'Operators/Assemblers', 'Armed Forces', 'Elementary']\n",
    "\n",
    "# Test Occupations were converted\n",
    "assert(all((clean_df.Occupation.value_counts().index == isco)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Combine Loan Status Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.LoanStatus = clean_df.LoanStatus.replace({'Past Due (1-15 days)': 'Past Due',\n",
    "                        'Past Due (31-60 days)': 'Past Due',\n",
    "                        'Past Due (61-90 days)': 'Past Due',\n",
    "                        'Past Due (91-120 days)': 'Past Due',\n",
    "                        'Past Due (16-30 days)': 'Past Due',\n",
    "                        'Past Due (>120 days)': 'Past Due',\n",
    "                        'FinalPaymentInProgress': 'Completed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_statuses = ['Current', 'Completed', 'Chargedoff', 'Defaulted',\n",
    "        'Past Due', 'Cancelled']\n",
    "\n",
    "# Test loan statuses were converted\n",
    "assert(all((clean_df.LoanStatus.value_counts().index == loan_statuses)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
